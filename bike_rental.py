# -*- coding: utf-8 -*-
"""bike_rental.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QL_UV8wzHrRk1SFRYgOIhDB73jYUdXtF
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv('Dataset.csv')

df.info()

df.head()

df['season']=df['season'].replace('springer','spring')

df=df.drop(['instant','casual','registered'], axis=1)

df.describe()

print((df=='?').sum())

df=df.replace('?',np.nan)
print((df=='?').sum())

df.head()

df['dteday']=pd.to_datetime(df['dteday'], format='%d-%m-%Y')

df.dtypes

numerical_columns= ['temp','atemp','hum','windspeed','mnth','yr','cnt']
 categorical_columns=['season','holiday','workingday','weathersit']

for col in numerical_columns:
  df[col]=pd.to_numeric(df[col],errors="coerce")
  df[col].fillna(df[col].median(),inplace=True)

for col in ['yr', 'mnth', 'cnt']:
    df[col] = df[col].astype(int)

df.dtypes

for col in categorical_columns:
  df[col].fillna(df[col].mode()[0],inplace=True)

df.head()

duplicates = df.duplicated().sum()

datetime_duplicates = df.duplicated(subset=['dteday', 'hr']).sum()
print(f"Duplicates on (dteday, hr): {datetime_duplicates}")

if duplicates > 0:
    df = df.drop_duplicates()
df.shape
missing_final = df.isnull().sum().sum()
print(f"Final missing values: {missing_final}")

def get_outliers(column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower) | (df[column] > upper)][column]
    return outliers, lower, upper

for col in ['cnt', 'temp', 'atemp', 'hum', 'windspeed']:
    outliers, lower, upper = get_outliers(col)
    percentage_outliers = (len(outliers) / len(df)) * 100
    print(f"  {col}: {len(outliers):,} ({percentage_outliers:.1f}%)")

for col in ['cnt', 'hum', 'windspeed']:
    outliers_data, lower, upper = get_outliers(col)
    df[col] = np.where(df[col] > upper, upper, np.where(df[col] < lower, lower, df[col]))
    print(f"Outliers in {col} after capping: {len(df[(df[col] < lower) | (df[col] > upper)][col])}")

for col in ['cnt', 'temp', 'hum', 'windspeed']:
  plt.figure(figsize=(8, 5))
  sns.boxplot(df[col])
  plt.title(f'Outlier Detection of {col} After Capping')
  plt.tight_layout()
  plt.show()

sns.pairplot(df[numerical_columns])
plt.show()

df_bike_plot = df.dropna(subset=numerical_columns)

plt.figure(figsize=(20, 15))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(3, 3, i)
    sns.histplot(df_bike_plot[col], kde=True, bins=30)
    plt.title(col)
plt.tight_layout()
plt.show()

for col in categorical_columns:
    plt.figure(figsize=(8,6))
    sns.boxplot(x=col, y='cnt', data=df)
    plt.title(f'{col} vs Count of Rentals')
    plt.show()

plt.figure(figsize=(8, 6))
corr = df[numerical_columns].corr()
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation of numerical columns")
plt.show()

for col in categorical_columns:
    plt.figure(figsize=(6,4))
    sns.countplot(x=col, data=df)
    plt.title(f'Count of each {col} category')
    plt.show()

cnt_by_date = df.groupby("dteday")["cnt"].sum().reset_index()

hourly_avg = df.groupby("hr")["cnt"].median().reset_index()
plt.figure(figsize=(10, 4))
sns.lineplot(data=hourly_avg, x="hr", y="cnt")
plt.title("Average Bike Rentals by Hour of Day")
plt.xlabel("Hour of Day")
plt.ylabel("Average Bike Rentals")
plt.tight_layout()
plt.show()

plt.figure(figsize=(20, 10))
sns.lineplot(x=df['dteday'], y=df['cnt'], color='red')
plt.title("Total Bike Rentals Over Time")
plt.xlabel("Date")
plt.ylabel("Total Rentals")
plt.show()

monthly = df.groupby('mnth')['cnt'].mean()
monthly.plot(kind='bar', figsize=(8,4), color='skyblue')
plt.title("Average Rentals per Month")
plt.xlabel("Month")
plt.ylabel("Average Count")
plt.show()

season_avg = df.groupby('season')['cnt'].mean().sort_index()
season_avg.plot(kind='bar', figsize=(6,4), color='Red')
plt.title("Average Rentals by Season")
plt.ylabel("Average Count")
plt.show()

df["dteday"] = pd.to_datetime(df["dteday"], errors="coerce")

df["day"] = df["dteday"].dt.day
df["quarter"] = df["dteday"].dt.quarter
df["is_weekend"] = (df["weekday"] >= 5).astype(int)

df["temp_sq"] = df["temp"] ** 2
df["hum_temp_interaction"] = df["hum"] * df["temp"]
df["windspeed_log"] = np.log1p(df["windspeed"])
df.head()

from sklearn.preprocessing import OneHotEncoder
categorical_cols = ["season", "weathersit", "holiday", "weekday", "workingday"]

encoder = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
encoded = encoder.fit_transform(df[categorical_cols])
encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical_cols))

df = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)
df.head()

from sklearn.preprocessing import StandardScaler

num_cols = ["temp", "atemp", "hum", "windspeed","day", "quarter", "temp_sq", "hum_temp_interaction", "windspeed_log"]

scaler = StandardScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])
df.head()

df["is_peak_hour"] = df["hr"].apply(lambda x: 1 if (7 <= x <= 9) or (17 <= x <= 19) else 0)
df["is_high_season"] = ((df["season_summer"] == 1) | (df["season_fall"] == 1)).astype(int)
df["comfort_index"] = df["temp"] * (1 - df["hum"])
df["is_weekend_or_holiday"] = ((df["is_weekend"] == 1) | (df["holiday_Yes"] == 1)).astype(int)
df["bad_weather"] = ((df["weathersit_Heavy Rain"] == 1) | (df["weathersit_Light Snow"] == 1)).astype(int)
df[["hr","is_peak_hour","is_high_season","comfort_index","is_weekend_or_holiday","bad_weather"]].head()



duplicates = df.duplicated().sum()

datetime_duplicates = df.duplicated(subset=['dteday', 'hr']).sum()
print(f"Duplicates on (dteday, hr): {datetime_duplicates}")

if duplicates > 0:
    df = df.drop_duplicates()
df.shape
missing_final = df.isnull().sum().sum()
print(f"Final missing values: {missing_final}")

df.to_csv('cleaned_bike_rental_data.csv', index=False)

data=pd.read_csv('cleaned_bike_rental_data.csv')

df.head()

df.info()

df.describe()

data.dtypes

data

"""4.	Model Building:
•	Selecting appropriate machine learning algorithms such as Decision Tree, Random Forest, and Gradient Boosting Regression for predicting bike rental demand.
•	Splitting the dataset into training and testing sets for model evaluation.
•	Training various models using the training data and evaluating their performance using metrics like RMSE, MAE, or R-squared.
•	Fine-tuning model parameters to optimize performance.

"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import root_mean_squared_error,mean_absolute_error,r2_score, mean_squared_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV

data["dteday"] = pd.to_datetime(data["dteday"])
data["timestamp_ns"] = data["dteday"].astype("int64")

data.head()

x = data.drop(['cnt', 'dteday'], axis=1)
y = data['cnt']

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

print("training shape",x_train.shape)
print("testing shape",x_test.shape)

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

"""Linear Regression Model"""

linear_model=LinearRegression()
linear_model.fit(x_train_scaled,y_train)
print(linear_model.intercept_)
print(linear_model.coef_)

train_predict=linear_model.predict(x_train_scaled)
test_predict=linear_model.predict(x_test_scaled)

train_RMSE=np.sqrt(mean_squared_error(y_train,train_predict))
test_RMSE=np.sqrt(mean_squared_error(y_test,test_predict))
train_r2 = r2_score(y_train, train_predict)
test_r2 = r2_score(y_test, test_predict)
test_mae = mean_absolute_error(y_test, test_predict)
print(f"Train RMSE: {train_RMSE:.4f}")
print(f"Test RMSE: {test_RMSE:.4f}")
print(f"Train r2: {train_r2:.2f}")
print(f"Test r2: {test_r2:.2f}")
print(f"Test mae:{test_mae:.2f}")

models_comparison= [{
    'Model': 'Linear Regression',
    'Train_R2': train_r2,
    'Test_R2': test_r2,
    'Train_RMSE': train_RMSE,
    'Test_RMSE': test_RMSE,
    'Test_MAE': test_mae
}]

"""Lasso Regression

"""

from sklearn.linear_model import Lasso
Lasso_model=Lasso()
Lasso_model.fit(x_train_scaled,y_train)
print(Lasso_model.intercept_)
print(Lasso_model.coef_)

lasso_train_predict=Lasso_model.predict(x_train_scaled)
lasso_test_predict=Lasso_model.predict(x_test_scaled)

lasso_train_RMSE=np.sqrt(mean_squared_error(lasso_train_predict,y_train))
lasso_test_RMSE=np.sqrt(mean_squared_error(lasso_test_predict,y_test))
lasso_train_r2 = r2_score(y_train, lasso_train_predict)
lasso_test_r2 = r2_score(y_test, lasso_test_predict)
lasso_test_mae = mean_absolute_error(y_test, lasso_test_predict)
print(f"Train RMSE: {lasso_train_RMSE:.4f}")
print(f"Test RMSE: {lasso_test_RMSE:.4f}")
print(f"Train r2: {lasso_train_r2:.3f}")
print(f"Test r2: {lasso_test_r2:.3f}")
print(f"Test mae:{lasso_test_mae:.2f}")

models_comparison.append({
    'Model': 'Lasso Regression',
    'Train_R2': lasso_train_r2,
    'Test_R2': lasso_test_r2,
    'Train_RMSE': lasso_train_RMSE,
    'Test_RMSE': lasso_test_RMSE,
    'Test_MAE': lasso_test_mae
})

"""Ridge Regression Model"""

from sklearn.linear_model import Ridge
ridge_model=Ridge()
ridge_model.fit(x_train_scaled,y_train)
print(ridge_model.intercept_)
print(ridge_model.coef_)

ridge_train_predict=ridge_model.predict(x_train_scaled)
ridge_test_predict=ridge_model.predict(x_test_scaled)

ridge_train_RMSE=np.sqrt(mean_squared_error(ridge_train_predict,y_train))
ridge_test_RMSE=np.sqrt(mean_squared_error(ridge_test_predict,y_test))
ridge_train_r2 = r2_score(y_train, ridge_train_predict)
ridge_test_r2 = r2_score(y_test, ridge_test_predict)
ridge_test_mae = mean_absolute_error(y_test, ridge_test_predict)
print(f"Train RMSE: {ridge_train_RMSE:.4f}")
print(f"Test RMSE: {ridge_test_RMSE:.4f}")
print(f"Train r2: {ridge_train_r2:.3f}")
print(f"Test r2: {ridge_test_r2:.3f}")
print(f"Test mae:{ridge_test_mae:.2f}")

models_comparison.append({
    'Model': 'Ridge Regression',
    'Train_R2': ridge_train_r2,
    'Test_R2': ridge_test_r2,
    'Train_RMSE': ridge_train_RMSE,
    'Test_RMSE': ridge_test_RMSE,
    'Test_MAE': ridge_test_mae
})

"""Decision Tree Model"""

#model building decision tree
dt_model=DecisionTreeRegressor(max_depth=10, min_samples_split=5,
                                 min_samples_leaf=2, random_state=42)
dt_model.fit(x_train_scaled,y_train)

dt_train_pred = dt_model.predict(x_train_scaled)
dt_test_pred = dt_model.predict(x_test_scaled)

dt_train_r2 = r2_score(y_train, dt_train_pred)
dt_test_r2 = r2_score(y_test, dt_test_pred)
dt_train_rmse = np.sqrt(mean_squared_error(y_train, dt_train_pred))
dt_test_rmse = np.sqrt(mean_squared_error(y_test, dt_test_pred))
dt_test_mae = mean_absolute_error(y_test, dt_test_pred)

print("Decision tree model performance")
print(f"Train RMSE: {dt_train_rmse:.4f}")
print(f"Test RMSE:  {dt_test_rmse:.4f}")
print(f"Train R²:   {dt_train_r2:.3f}")
print(f"Test R²:    {dt_test_r2:.3f}")
print(f"Test MAE:   {dt_test_mae:.2f}")

"""Feature importance graph for decision tree model"""

importance = dt_model.feature_importances_
features = x_train.columns
plt.figure(figsize=(10,10))
plt.barh(features, importance)
plt.xlabel("Feature Importance")
plt.title("Decision Tree Feature Importance")
plt.show()

models_comparison.append({
    'Model': 'Decision Tree',
    'Train_R2': dt_train_r2,
    'Test_R2': dt_test_r2,
    'Train_RMSE': dt_train_rmse,
    'Test_RMSE': dt_test_rmse,
    'Test_MAE': dt_test_mae
})

"""Random Forest"""



#model building
from sklearn.ensemble import RandomForestRegressor
rf_model=RandomForestRegressor(n_estimators=100, max_depth=15,
                                  min_samples_split=5, min_samples_leaf=2,
                                  random_state=42, n_jobs=-1)
rf_model.fit(x_train_scaled,y_train)

#predictions
rf_train_pred = rf_model.predict(x_train_scaled)
rf_test_pred = rf_model.predict(x_test_scaled)

#evalauating performance of the model
rf_train_r2 = r2_score(y_train, rf_train_pred)
rf_test_r2 = r2_score(y_test, rf_test_pred)
rf_train_rmse = np.sqrt(mean_squared_error(y_train, rf_train_pred))
rf_test_rmse = np.sqrt(mean_squared_error(y_test, rf_test_pred))
rf_test_mae = mean_absolute_error(y_test, rf_test_pred)

print("Random Forest model performance")
print(f"Train RMSE: {rf_train_rmse:.4f}")
print(f"Test RMSE:  {rf_test_rmse:.4f}")
print(f"Train R²:   {rf_train_r2:.3f}")
print(f"Test R²:    {rf_test_r2:.3f}")
print(f"Test MAE:   {rf_test_mae:.2f}")

models_comparison.append({
    'Model': 'Random Forest',
    'Train_R2': rf_train_r2,
    'Test_R2': rf_test_r2,
    'Train_RMSE': rf_train_rmse,
    'Test_RMSE': rf_test_rmse,
    'Test_MAE': rf_test_mae
})

"""Gradient Boosting"""

from sklearn.ensemble import GradientBoostingRegressor
gb_model=GradientBoostingRegressor(n_estimators=200,learning_rate=0.1,random_state=42)
gb_model.fit(x_train_scaled,y_train)

#predictions
gb_train_pred = gb_model.predict(x_train_scaled)
gb_test_pred = gb_model.predict(x_test_scaled)

#evaluating performance of the model
gb_train_r2 = r2_score(y_train, gb_train_pred)
gb_test_r2 = r2_score(y_test, gb_test_pred)
gb_train_rmse = np.sqrt(mean_squared_error(y_train, gb_train_pred))
gb_test_rmse = np.sqrt(mean_squared_error(y_test, gb_test_pred))
gb_test_mae = mean_absolute_error(y_test, gb_test_pred)

print("Gradient Boosting model performance")
print("Gradient Boosting model performance")
print(f"Train RMSE: {gb_train_rmse:.4f}")
print(f"Test RMSE:  {gb_test_rmse:.4f}")
print(f"Train R²:   {gb_train_r2:.3f}")
print(f"Test R²:    {gb_test_r2:.3f}")
print(f"Test MAE:   {gb_test_mae:.2f}")

models_comparison.append({
    'Model': 'Gradient Boosting',
    'Train_R2': gb_train_r2,
    'Test_R2': gb_test_r2,
    'Train_RMSE': gb_train_rmse,
    'Test_RMSE': gb_test_rmse,
    'Test_MAE': gb_test_mae
})

"""XGBoost Model"""

from xgboost import XGBRegressor

xgb_model = XGBRegressor(n_estimators=100,learning_rate=0.1,random_state=42,n_jobs=-1,verbosity=0)

xgb_model.fit(x_train_scaled, y_train)

xgb_train_pred = xgb_model.predict(x_train_scaled)
xgb_test_pred = xgb_model.predict(x_test_scaled)

xgb_test_rmse = np.sqrt(mean_squared_error(y_test, xgb_test_pred))
xgb_test_mae = mean_absolute_error(y_test, xgb_test_pred)
xgb_train_r2 = r2_score(y_train, xgb_train_pred)
xgb_test_r2 = r2_score(y_test, xgb_test_pred)
xgb_train_rmse = np.sqrt(mean_squared_error(y_train, xgb_train_pred))

print(f"Train RMSE:  {xgb_train_rmse:.4f}")
print(f"Test RMSE:   {xgb_test_rmse:.4f}")
print(f"Train R²:    {xgb_train_r2:.3f}")
print(f"Test R²:     {xgb_test_r2:.3f}")
print(f"Test MAE:    {xgb_test_mae:.2f}")

models_comparison.append({
    'Model': 'XGBoost',
    'Train_R2': xgb_train_r2,
    'Test_R2': xgb_test_r2,
    'Train_RMSE': xgb_train_rmse,
    'Test_RMSE': xgb_test_rmse,
    'Test_MAE': xgb_test_mae
})

"""Lightgbm Model"""

from lightgbm import LGBMRegressor
lgbm_model = LGBMRegressor(n_estimators=100,learning_rate=0.1,random_state=42,n_jobs=-1,verbose=-1 )
lgbm_model.fit(x_train_scaled, y_train)

lgb_train_pred=lgbm_model.predict(x_train_scaled)
lgb_test_pred=lgbm_model.predict(x_test_scaled)

lgb_train_rmse = np.sqrt(mean_squared_error(y_train, lgb_train_pred))
lgb_test_rmse = np.sqrt(mean_squared_error(y_test, lgb_test_pred))
lgb_train_r2 = r2_score(y_train, lgb_train_pred)
lgb_test_r2 = r2_score(y_test, lgb_test_pred)
lgb_test_mae = mean_absolute_error(y_test, lgb_test_pred)

print(f"Train RMSE:  {lgb_train_rmse:.4f}")
print(f"Test RMSE:   {lgb_test_rmse:.4f}")
print(f"Train R²:    {lgb_train_r2:.3f}")
print(f"Test R²:     {lgb_test_r2:.3f}")
print(f"Test MAE:    {lgb_test_mae:.2f}")

models_comparison.append({
    'Model': 'LightGBM',
    'Train_R2': lgb_train_r2,
    'Test_R2': lgb_test_r2,
    'Train_RMSE': lgb_train_rmse,
    'Test_RMSE': lgb_test_rmse,
    'Test_MAE': lgb_test_mae
})

"""Model Comparsion

"""

comparison=pd.DataFrame({
    'Model': ['Linear Regression','Lasso Regression','Ridge Regression','Decision Tree','Random Forest','Gradient Boosting','XGBoost','LightGBM'],
    'Train_RMSE': [train_RMSE,lasso_train_RMSE,ridge_train_RMSE,dt_train_rmse,rf_train_rmse,gb_train_rmse,xgb_train_rmse,lgb_train_rmse],
    'Test_RMSE': [test_RMSE,lasso_test_RMSE,ridge_test_RMSE,dt_test_rmse,rf_test_rmse,gb_test_rmse,xgb_test_rmse,lgb_test_rmse],
    'MAE': [test_mae,lasso_test_mae,ridge_test_mae,dt_test_mae,rf_test_mae,gb_test_mae,xgb_test_mae,lgb_test_mae],
    'Train_R2': [train_r2,lasso_train_r2,ridge_train_r2,dt_train_r2,rf_train_r2,gb_train_r2,xgb_train_r2,lgb_train_r2],
    'Test_R2': [test_r2,lasso_test_r2,ridge_test_r2,dt_test_r2,rf_test_r2,gb_test_r2,xgb_test_r2,lgb_test_r2]
})

comparison

comparison.describe()

"""From the above models we can conclude that the random forest is the best model for the given dataset.

Random forest model has minimum errors as we can see the rmse and mae are lesser than the other models and it has the highest r2 score.

In decision tree model,simple and interpretable as it has large errors and overfits easily. as we can see tha rmse is higher than the random forest

Gradient boosting model has a high mae value which is 2574.66 the reason behind this higher mae value is needs proper preprocessing,scaling or hyperparameter tuning.

lightGBM model is the best for the given dataset.because it has less mae score which is 23.1 and it is the best model for the given dataset.

After comparing three regression models, Random Forest outperformed both Decision Tree and Gradient Boosting with an RMSE of 14.27, MAE of 8.63, and R² of 0.99. Decision Tree performed moderately well but had higher errors, while Gradient Boosting showed potential but produced extremely high MAE, indicating issues in model scaling or data preprocessing.

We evaluate the final model using Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and the R² score. These metrics provide a quantitative measure of the model's performance, confirming its reliability for predicting bike rental demand.

LightGBM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from lightgbm import LGBMRegressor

# 1. Define Parameter Grids for Hyperparameter Tuning
lgb_params = {
    'n_estimators': [100, 500],
    'learning_rate': [0.01, 0.05, 0.1],
    'num_leaves': [31, 50, 100],
    'max_depth': [-1, 10, 20],
    'subsample': [0.8, 1.0]
}

rf_params = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5]
}

# : Parameter grid for XGBoost
xgb_params = {
    'n_estimators': [100, 500],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 6, 10],
    'subsample': [0.7, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.9, 1.0]
}

# 2. Hyperparameter Tuning using RandomizedSearchCV
print("Tuning LightGBM...")
lgb_cv = RandomizedSearchCV(LGBMRegressor(random_state=42, verbose=-1), lgb_params,
                            n_iter=10, cv=3, scoring='r2', n_jobs=-1, random_state=42)
lgb_cv.fit(x_train_scaled, y_train)
best_lgbm = lgb_cv.best_estimator_

print("Tuning Random Forest...")
rf_cv = RandomizedSearchCV(RandomForestRegressor(random_state=42), rf_params,
                            n_iter=10, cv=3, scoring='r2', n_jobs=-1, random_state=42)
rf_cv.fit(x_train_scaled, y_train)
best_rf = rf_cv.best_estimator_

print("Tuning XGBoost...")
# New: XGBoost Tuning Step
xgb_cv = RandomizedSearchCV(XGBRegressor(random_state=42, objective='reg:squarederror'), xgb_params,
                            n_iter=10, cv=3, scoring='r2', n_jobs=-1, random_state=42)
xgb_cv.fit(x_train_scaled, y_train)
best_xgb = xgb_cv.best_estimator_

# 3. Model Evaluation and Comparison
def get_performance(model, name):
    preds = model.predict(x_test_scaled)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    mae = mean_absolute_error(y_test, preds)
    r2 = r2_score(y_test, preds)
    return {'Model': name, 'RMSE': rmse, 'MAE': mae, 'R2': r2}

results_list = [
    get_performance(best_lgbm, "Tuned LightGBM"),
    get_performance(best_rf, "Tuned Random Forest"),
    get_performance(best_xgb, "Tuned XGBoost")
]

comparison_df = pd.DataFrame(results_list).sort_values(by='R2', ascending=False)
print("\n--- Model Comparison ---")
print(comparison_df)

# Automatically select the best model for the visualizations
best_model_name = comparison_df.iloc[0]['Model']
# Map the string name back to the model object
model_map = {
    "Tuned LightGBM": best_lgbm,
    "Tuned Random Forest": best_rf,
    "Tuned XGBoost": best_xgb
}
best_model = model_map[best_model_name]

# 4. Visualizations for Predictions
final_preds = best_model.predict(x_test_scaled)

# Plot 1: Actual vs Predicted
plt.figure(figsize=(19, 4))
plt.subplot(1, 2, 1)
sns.scatterplot(x=y_test, y=final_preds, alpha=0.6, color='teal')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.title(f'Actual vs Predicted ({best_model_name})')
plt.xlabel('Actual Count')
plt.ylabel('Predicted Count')

# Plot 2: Residuals Distribution
plt.figure(figsize=(18, 6))
plt.subplot(1, 2, 2)
residuals = y_test - final_preds
sns.histplot(residuals, kde=True, color='purple')
plt.axvline(0, color='red', linestyle='--')
plt.title(f'Residuals Distribution ({best_model_name})')
plt.xlabel('Prediction Error')
plt.tight_layout()
plt.show()

"""DEPLOYMENT

"""

X = data.drop(['cnt', 'dteday'], axis=1)
y = data['cnt']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42
)

print(X.columns.tolist())

final_features = [
    'yr', 'mnth', 'hr', 'temp', 'atemp', 'hum', 'windspeed',
    'day', 'quarter', 'is_weekend', 'temp_sq',
    'hum_temp_interaction', 'windspeed_log',
    'season_fall', 'season_spring', 'season_summer', 'season_winter',
    'weathersit_Clear', 'weathersit_Heavy Rain', 'weathersit_Light Snow', 'weathersit_Mist',
    'holiday_No', 'holiday_Yes',
    'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6',
    'workingday_No work', 'workingday_Working Day',
    'is_peak_hour', 'is_high_season', 'comfort_index',
    'is_weekend_or_holiday', 'bad_weather', 'timestamp_ns'
]

X = data[final_features]
y = data['cnt']

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = RandomForestRegressor(
    n_estimators=200,
    random_state=42
)

model.fit(X_train, y_train)

import pickle

with open("bike_demand_model.pkl", "wb") as f:

    pickle.dump(model, f)

from google.colab import files

files.download("bike_demand_model.pkl")

"""The dataset had already undergone extensive feature engineering, including one-hot encoding and derived features. To ensure consistency between training and inference, I aligned deployment with the engineered feature space rather than retraining on raw variables

# OUTPUT GRAPH

# Target Variable Distribution
"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
sns.histplot(data['cnt'], bins=30, kde=True)
plt.title("Distribution of Bike Rental Demand")
plt.xlabel("Bike Count")
plt.ylabel("Frequency")
plt.show()

"""Bike demand is right-skewed, indicating higher rentals during peak periods

# Hour vs Average Bike Demand
"""

plt.figure(figsize=(8,5))
sns.lineplot(x='hr', y='cnt', data=data, estimator='mean')
plt.title("Average Bike Demand by Hour")
plt.xlabel("Hour of Day")
plt.ylabel("Average Demand")
plt.show()

"""Morning and evening hours show peak demand due to office commute

# WEATHER VS BIKE DEMAND
"""

data['weather'] = (
    data[['weathersit_Clear',
          'weathersit_Mist',
          'weathersit_Light Snow',
          'weathersit_Heavy Rain']]
    .idxmax(axis=1)
    .str.replace('weathersit_', '')
)
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(7,5))
sns.barplot(x='weather', y='cnt', data=data)
plt.title("Bike Demand by Weather Condition")
plt.xlabel("Weather Condition")
plt.ylabel("Average Bike Demand")
plt.show()

"""Since categorical variables were one-hot encoded, I reconstructed the original weather categories for visualization purposes.

Clear weather results in maximum bike usage

# Season vs Bike Demand
"""

data['season'] = (
    data[['season_spring', 'season_summer',
          'season_fall', 'season_winter']]
    .idxmax(axis=1)
    .str.replace('season_', '')
)

sns.barplot(x='season', y='cnt', data=data)

"""Summer and fall have higher demand due to favorable weather.

# Correlation Heatmap
"""

numeric_data = data.select_dtypes(include=['int64', 'float64'])
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12,8))
corr = numeric_data.corr()

sns.heatmap(corr, cmap='coolwarm', annot=False)
plt.title("Feature Correlation Heatmap")
plt.show()

"""Correlation analysis was performed only on numerical features, as correlation is mathematically defined for numeric variables

Temperature and hour show strong correlation with bike demand.

# Actual vs Predicted Values
"""

y_pred = model.predict(X_test)

plt.figure(figsize=(7,5))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel("Actual Bike Demand")
plt.ylabel("Predicted Bike Demand")
plt.title("Actual vs Predicted Bike Demand")
plt.show()

"""Predicted values closely follow actual values, indicating good model fit.

# Residual Plot
"""

residuals = y_test - y_pred

plt.figure(figsize=(7,5))
sns.histplot(residuals, bins=30, kde=True)
plt.title("Residual Distribution")
plt.xlabel("Residuals")
plt.show()

"""Residuals are centered around zero, showing minimal bias"""